# -*- coding: utf-8 -*-
"""Deep Learning Model on IMDB Movie Reviews

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lFAF872Y7iG2dIpEJ8UlflcDrnd87WIY
"""

import tensorflow as tf

import nltk
import numpy as np
import pandas as pd
import io
import string
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from google.colab import files
from bs4 import BeautifulSoup
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import SnowballStemmer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras import regularizers
nltk.download('stopwords')
nltk.download('punkt')

"""##**Collecting Data**"""

# Loaded in the all the movie reviews data stored on my computer
uploaded = files.upload()

# storing dataset as a Pandas Dataframe
data = pd.read_csv(io.BytesIO(uploaded["IMDB Dataset (5).csv"]))

data.describe()

data.isnull().sum()

# Checking out the data
print(data.head())
print(data.shape)
print(data['sentiment'].value_counts())

"""Dataset is balanced.


"""

reviews = data['review']
print(reviews.head())
print(reviews.shape)
sentiment = data['sentiment']
print(sentiment.head())
print(sentiment.shape)

"""## **PreProcessing The Data For ML**"""

def preprocess_text(review):

    #remove html tags
    soup = BeautifulSoup(review, 'html.parser')
    review_with_no_html = soup.get_text()
    #tokenizing the words, breaking up a sentence into [] of words
    words = word_tokenize(review_with_no_html)
    #remove punctuation
    punctuation = set(string.punctuation)
    words = [word for word in words if word not in punctuation]
    #remove stopwords
    sw = set(stopwords.words("english"))
    words = [word for word in words if word.lower() not in sw]
    #remove non-alphabetic words
    words = [word for word in words if word.isalpha()]
    #using SnowBallStemmer to stem each word
    #For example, the words 'walk', 'walked', 'walks' or 'walking' will be all converted to the base form 'walk'
    snowball = SnowballStemmer(language = 'english')
    stemmed_words = [snowball.stem(word) for word in words]

    preprocessed_review = ' '.join(stemmed_words)
    return preprocessed_review

preprocessed_reviews = reviews.apply(preprocess_text)

print(reviews.head())
print(preprocessed_reviews.head())

"""## **Prepare Data For Neutral Network**"""

# will ignore words that appear in less than 0.102% of the Reviews and more than 70% of the Reviews
bag_of_words_tool = CountVectorizer(min_df=0.00102,max_df=0.7)

# now we have a bunch of words, bag of words will count how many of each word is in the sentence. Ex: 'the' appears 4 times in this sentence
preprocessed_reviews_with_bagOfWords = bag_of_words_tool.fit_transform(preprocessed_reviews)

print(preprocessed_reviews_with_bagOfWords.shape)
# the type is now a sparse matrix have to convert back to Dataframe
print(type(preprocessed_reviews_with_bagOfWords))

# converting back to Dataframe
data_is_ready = pd.DataFrame.sparse.from_spmatrix(preprocessed_reviews_with_bagOfWords)
print(data_is_ready.shape)
print(type(data_is_ready))

# label encoder, is just labeling our sentiment as 1 or 0, for example positive=1 and negative=0
label_encoder = LabelEncoder()
encoded_sentiment = label_encoder.fit_transform(sentiment)
print(encoded_sentiment.shape)
print(encoded_sentiment)

# spliting all of the data into training and testing set, 50& will go to training and 50% will be saved for testing
x_train, x_test, y_train, y_test = train_test_split(data_is_ready, encoded_sentiment, random_state=3, train_size=0.50)
print('training_set x size: ', x_train.shape)
print('training_set y size: ', y_train.shape)
print('testing_set x size: ', x_test.shape)
print('testing_set y size: ', y_test.shape)

"""## **Time To Apply Neutral Network**"""

# Now we will create a Neural Network Model! configuring it like how many layers (I have set it to 5), and how many neutrons in each layers ..
# these hyperparameter need to be iterately modified after testing to optimize to imporve performance

model = tf.keras.Sequential ([
    tf.keras.layers.Dense (256, activation = 'relu',
                           input_shape = (x_train.shape[1],),
                           kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.001, l2=0.025)),
    tf.keras.layers.Dense (128, activation = 'relu'),
    tf.keras.layers.Dense (64, activation = 'relu'),
    tf.keras.layers.Dense (32, activation = 'relu'),
    tf.keras.layers.Dense (2, activation = 'softmax')
])

# Adam optimization: a gradient descent method that is "computationally efficient, has little memory requirement, is well suited for problems that are large in terms of data/parameters"
# Use this SparseCategoricalCrossentropy crossentropy loss function when there are two or more label classes.
# Accuracy metric: calculates how often predictions equal labels.
model.compile (tf.keras.optimizers.Adam (learning_rate = 0.0001),
               loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits =True),
               metrics = ['accuracy'])

model.summary ()

"""Total params: 1,816,098
What does this mean? It is # of links between the neutons, each link has different 'weights' and 'bais'. It is interesting because the Neutral Networks Deep Learning is creating these links themselves! kind of like making their own conclusions in a way "thinking"

Quick side comment: Humans brains have approx over trillions of connections.
"""

print(type(x_train))
print(type(y_train))

# calculating the loss function for each "epoch" and trying to adjust the model to optimize performance
history = model.fit (x_train.to_numpy(), y_train,
                          validation_data=(x_test.to_numpy(), y_test),
                          epochs = 5)
history

"""The training score is 88% and testing score is 87%, only 1% diff, therefore accurate model!

Next steps would be to keep on tuning the hyperparameters to find the highest training score with the keeping an acceptable diff with testing score
"""

# Plot training score vs testing score of model for each epoch
plt.figure(figsize=(8, 4), dpi=200)
with sns.axes_style('darkgrid'):
  ax1 = sns.lineplot(history.history['accuracy'], label='Training Accuracy', color='blue')
  ax2 = sns.lineplot(history.history['val_accuracy'], label='Testing Accuracy', color='red')
plt.title('Training vs Testing Accuracy Score')
plt.xlabel('Epochs')
plt.ylabel('Score (%)')
plt.legend()